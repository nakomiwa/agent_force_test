from base_prompt import BacePrompt
import mlflow
import datetime
import json
import pandas as pd
import os
from typing import Dict, Any

class CustomerSummarizePrompt(BacePrompt):
    def __init__(self, experiment_base_path: str = "/Workspace/b_poc/b_0048/experiments"):
        super().__init__(experiment_base_path)
        self.temperature = 0
    
    def _setup_mlflow_experiment(self):
        """MLflowå®Ÿé¨“ã®è¨­å®šã‚’è¡Œã†ï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…ï¼‰"""
        try:
            # å®Ÿé¨“ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            experiment = mlflow.get_experiment_by_name(self.experiment_path)
            
            if experiment is None:
                # å®Ÿé¨“ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
                print(f"ğŸ”„ MLflowå®Ÿé¨“ã‚’ä½œæˆä¸­: {self.experiment_path}")
                experiment_id = mlflow.create_experiment(self.experiment_path)
                mlflow.set_experiment(experiment_id=experiment_id)
                print(f"âœ… MLflowå®Ÿé¨“ã‚’ä½œæˆã—ã¾ã—ãŸ: {self.experiment_path}")
            else:
                # å®Ÿé¨“ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯è¨­å®š
                mlflow.set_experiment(experiment_id=experiment.experiment_id)
                print(f"âœ… æ—¢å­˜ã®MLflowå®Ÿé¨“ã‚’ä½¿ç”¨: {self.experiment_path}")
                
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å®Ÿé¨“åã§ç›´æ¥è¨­å®šã‚’è©¦è¡Œ
            print(f"âš ï¸ å®Ÿé¨“è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            try:
                print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–¹å¼ã§å®Ÿé¨“ã‚’è¨­å®šä¸­: {self.experiment_path}")
                mlflow.set_experiment(self.experiment_path)
                print(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šå®Œäº†: {self.experiment_path}")
            except Exception as fallback_error:
                print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šã‚‚å¤±æ•—ã—ã¾ã—ãŸ: {fallback_error}")
                # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ä¸€æ™‚çš„ãªå®Ÿé¨“åã‚’ä½¿ç”¨
                temp_experiment = f"/tmp/{self.class_name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
                print(f"ğŸ”„ ä¸€æ™‚çš„ãªå®Ÿé¨“ã‚’ä½œæˆ: {temp_experiment}")
                mlflow.set_experiment(temp_experiment)
    
    def load_customer_data(self) -> str:
        """é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’CSVã‹ã‚‰èª­ã¿è¾¼ã¿ã€æ–‡å­—åˆ—ã¨ã—ã¦è¿”ã™"""
        csv_path = os.path.join(self.config_dir, "customer_data.csv")
        
        if not os.path.exists(csv_path):
            print(f"âŒ é¡§å®¢ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
            if os.path.exists(self.config_dir):
                files = os.listdir(self.config_dir)
                print(f"ğŸ” Configãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å†…å®¹: {files}")
            return ""
        
        try:
            df = pd.read_csv(csv_path, encoding='utf-8')
            print(f"âœ… é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(df)}ä»¶")
            customer_data_text = df.to_string(index=False)
            return customer_data_text
            
        except Exception as e:
            print(f"âŒ é¡§å®¢ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def generate(self) -> str:
        """
        é¡§å®¢è¦ç´„ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…ï¼‰
        """
        print(f"ğŸ”„ {self.class_name}: é¡§å®¢è¦ç´„ç”Ÿæˆä¸­...")
        
        customer_data = self.load_customer_data()
        if not customer_data:
            print(f"âš ï¸ é¡§å®¢ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return ""
        
        prompt_data = self.load_yaml("prompt.yaml")
        prompt_template = prompt_data.get('prompt', '')
        
        if not prompt_template:
            print(f"âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚config/prompt.yamlã®{self.class_name}ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return ""

        if '{customer_data}' in prompt_template:
            prompt = prompt_template.format(customer_data=customer_data)
        else:
            prompt = f"{prompt_template}\n\né¡§å®¢ãƒ‡ãƒ¼ã‚¿:\n{customer_data}"
        
        response = self.llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature
        )
        
        answer = response.choices[0].message.content
        
        self.save_yaml({
            "answer": answer, 
            "generated_at": datetime.datetime.now().isoformat(),
            "prompt_preview": prompt[:100] + "..." if len(prompt) > 100 else prompt,
            "task_type": "é¡§å®¢è¦ç´„",
            "customer_data_preview": customer_data[:200] + "..." if len(customer_data) > 200 else customer_data,
            "customer_data_length": len(customer_data),
            "final_prompt_length": len(prompt)
        }, "answer.yaml")
        
        return answer
    
    def evaluate(self, answer: str = None) -> Dict[str, Any]:
        """
        å…±é€šè©•ä¾¡é …ç›® + å­ã‚¯ãƒ©ã‚¹å›ºæœ‰è©•ä¾¡é …ç›®ã‚’çµ±åˆã—ã¦è©•ä¾¡ï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…ï¼‰
        """
        print(f"ğŸ”„ {self.class_name}: çµ±åˆè©•ä¾¡å®Ÿè¡Œä¸­...")
        
        # å›ç­”ã®å–å¾—
        if answer is None:
            answer_data = self.load_yaml("answer.yaml")
            answer = answer_data.get('answer', '')
        
        if not answer:
            print(f"âš ï¸ è©•ä¾¡å¯¾è±¡ã®å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return {"error": "å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        
        # 1. å…±é€šè©•ä¾¡é …ç›®ã‚’å–å¾—
        common_data = self.load_common_yaml("prompt.yaml")
        common_eval_items = common_data.get('eval_items', [])
        
        # 2. å­ã‚¯ãƒ©ã‚¹å›ºæœ‰ã®è©•ä¾¡é …ç›®ã‚’å–å¾—
        class_data = self.load_yaml("prompt.yaml")
        class_eval_items = class_data.get('eval_items', [])
        
        # 3. è©•ä¾¡é …ç›®ã‚’çµ±åˆ
        all_eval_items = common_eval_items + class_eval_items
        
        if not all_eval_items:
            print(f"âš ï¸ è©•ä¾¡é …ç›®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return {"error": "è©•ä¾¡é …ç›®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        
        # 4. çµ±åˆè©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        eval_items_text = "\n".join([f"- {item}" for item in all_eval_items])
        
        eval_prompt_template = common_data.get('eval_prompt_template', '')
        if not eval_prompt_template:
            print(f"âš ï¸ è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return {"error": "è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        
        eval_prompt = eval_prompt_template.format(
            answer=answer,
            eval_items=eval_items_text
        )
        
        # 5. LLMã«çµ±åˆè©•ä¾¡ã‚’ã•ã›ã‚‹
        try:
            response = self.llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": eval_prompt}],
                temperature=0.0
            )
            
            evaluation_result = response.choices[0].message.content
            
            try:
                evaluation_data = json.loads(evaluation_result)
                print(f"âœ… çµ±åˆè©•ä¾¡å®Œäº†: {evaluation_data}")
            except json.JSONDecodeError:
                print(f"âš ï¸ è©•ä¾¡çµæœã®JSONè§£æã«å¤±æ•—ã—ã¾ã—ãŸ")
                evaluation_data = {"error": "è©•ä¾¡çµæœã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ", "raw_result": evaluation_result}
                
        except Exception as e:
            print(f"âŒ çµ±åˆè©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼: {e}")
            evaluation_data = {"error": f"çµ±åˆè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}"}
        
        # 6. è©•ä¾¡çµæœã‚’ä¿å­˜
        combined_evaluation = {
            "evaluation": evaluation_data,
            "common_eval_items": common_eval_items,
            "class_eval_items": class_eval_items,
            "evaluated_at": datetime.datetime.now().isoformat(),
            "eval_prompt_preview": eval_prompt[:100] + "..." if len(eval_prompt) > 100 else eval_prompt
        }
        
        self.save_yaml(combined_evaluation, "evaluation.yaml")
        
        # 7. MLFlowã«è¨˜éŒ²
        try:
            self._log_to_mlflow(answer, evaluation_data, class_data, common_data)
        except Exception as e:
            print(f"âš ï¸ MLflowè¨˜éŒ²ã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        return combined_evaluation
    
    def _log_to_mlflow(self, answer: str, evaluation_data: Dict[str, Any], prompt_data: Dict[str, Any], common_data: Dict[str, Any]):
        """
        CustomerSummarizePromptç”¨ã®MLflowãƒ­ã‚°
        """
        print(f"ğŸ”„ MLflowãƒ­ã‚°è¨˜éŒ²é–‹å§‹...")
        
        # å®Ÿé¨“è¨­å®š
        self._setup_mlflow_experiment()
        
        run_name = f"{self.class_name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        print(f"ğŸš€ MLflowå®Ÿè¡Œé–‹å§‹: {run_name}")
        
        with mlflow.start_run(run_name=run_name):
            # ============================================================
            # 1. åŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°
            # ============================================================
            mlflow.log_param("ã‚¯ãƒ©ã‚¹å", self.class_name)
            mlflow.log_param("å®Ÿè¡Œæ—¥æ™‚", datetime.datetime.now().isoformat())
            mlflow.log_param("ãƒ¢ãƒ‡ãƒ«å", "gpt-4o-mini")
            mlflow.log_param("Temperature", self.temperature)
            mlflow.log_param("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ", self.project_root)
            mlflow.log_param("å®Ÿé¨“ãƒ‘ã‚¹", self.experiment_path)
            
            # ============================================================
            # 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ­ã‚°
            # ============================================================
            if 'prompt' in prompt_data:
                mlflow.log_text(prompt_data['prompt'], f"{self.class_name}_ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ.txt")
            
            # å…±é€šè©•ä¾¡é …ç›®ã‚’ãƒ­ã‚°
            if 'eval_items' in common_data:
                common_items_text = "\n".join(common_data['eval_items'])
                mlflow.log_text(common_items_text, f"{self.class_name}_å…±é€šè©•ä¾¡é …ç›®.txt")
            
            # ã‚¯ãƒ©ã‚¹å›ºæœ‰è©•ä¾¡é …ç›®ã‚’ãƒ­ã‚°
            if 'eval_items' in prompt_data:
                class_items_text = "\n".join(prompt_data['eval_items'])
                mlflow.log_text(class_items_text, f"{self.class_name}_å›ºæœ‰è©•ä¾¡é …ç›®.txt")
            
            # ============================================================
            # 3. ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã‚’ãƒ­ã‚°
            # ============================================================
            mlflow.log_text(answer, f"{self.class_name}_å›ç­”.txt")
            mlflow.log_param("å›ç­”æ–‡å­—æ•°", len(answer))
            
            # ============================================================
            # 4. è©•ä¾¡çµæœã‚’ãƒ­ã‚°
            # ============================================================
            if isinstance(evaluation_data, dict) and 'error' not in evaluation_data:
                # å…±é€šè©•ä¾¡é …ç›®ã®ã‚¹ã‚³ã‚¢
                if 'ç°¡æ½”ã•' in evaluation_data:
                    mlflow.log_metric("å…±é€š_ç°¡æ½”ã•", evaluation_data['ç°¡æ½”ã•'])
                if 'ä¸€è²«æ€§' in evaluation_data:
                    mlflow.log_metric("å…±é€š_ä¸€è²«æ€§", evaluation_data['ä¸€è²«æ€§'])
                
                # ã‚¯ãƒ©ã‚¹å›ºæœ‰è©•ä¾¡é …ç›®ã®ã‚¹ã‚³ã‚¢
                if 'è¦ç´„ã®æ­£ç¢ºæ€§' in evaluation_data:
                    mlflow.log_metric("å›ºæœ‰_è¦ç´„ã®æ­£ç¢ºæ€§", evaluation_data['è¦ç´„ã®æ­£ç¢ºæ€§'])
                if 'å–¶æ¥­æœ‰ç”¨æ€§' in evaluation_data:
                    mlflow.log_metric("å›ºæœ‰_å–¶æ¥­æœ‰ç”¨æ€§", evaluation_data['å–¶æ¥­æœ‰ç”¨æ€§'])
                
                # è©•ä¾¡ç†ç”±ãŒã‚ã‚Œã°ãƒ­ã‚°
                if 'è©•ä¾¡ç†ç”±' in evaluation_data:
                    mlflow.log_text(str(evaluation_data['è©•ä¾¡ç†ç”±']), f"{self.class_name}_è©•ä¾¡ç†ç”±.txt")
            
            print(f"âœ… MLflowå®Ÿè¡Œå®Œäº†: {run_name}")
            print(f"   - è©•ä¾¡çµæœ: {evaluation_data}")