from base_prompt import BacePrompt
import mlflow
import datetime
import json
from typing import Dict, Any

class CustomerSummarizePrompt(BacePrompt):
    def __init__(self, experiment_base_path: str = "/Workspace/b_poc/b_0048/experiments"):
        super().__init__(experiment_base_path)
        self.temperature = 0
    
    def generate(self):
        print(f"ğŸ”„ {self.class_name}: é¡§å®¢è¦ç´„ç”Ÿæˆä¸­...")
        
        prompt_data = self.load_yaml("prompt.yaml")
        prompt = prompt_data.get('prompt', '')
        
        if not prompt:
            print(f"âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚config/prompt.yamlã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return ""
        
        response = self.llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature
        )
        
        answer = response.choices[0].message.content
        
        self.save_yaml({
            "answer": answer, 
            "generated_at": datetime.datetime.now().isoformat(),
            "prompt_preview": prompt[:100] + "..." if len(prompt) > 100 else prompt,
            "task_type": "é¡§å®¢è¦ç´„"
        }, "answer.yaml")
        
        return answer
    
    def evaluate(self):
        print(f"ğŸ”„ {self.class_name}: é¡§å®¢è¦ç´„è©•ä¾¡ä¸­...")
        
        # generateãƒ¡ã‚½ãƒƒãƒ‰ã§ç”Ÿæˆã—ãŸå›ç­”ã‚’ãƒ­ãƒ¼ãƒ‰
        answer_data = self.load_yaml("answer.yaml")
        answer = answer_data.get('answer', '')
        
        if not answer:
            print(f"âš ï¸ è©•ä¾¡å¯¾è±¡ã®å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«generate()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            return {"error": "å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        
        # è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
        prompt_data = self.load_yaml("prompt.yaml")
        eval_prompt_template = prompt_data.get('eval_prompt', '')
        
        if not eval_prompt_template:
            print(f"âš ï¸ è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚config/prompt.yamlã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return {"error": "è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        
        eval_prompt = eval_prompt_template.format(answer=answer)
        
        # LLMã«è©•ä¾¡ã•ã›ã‚‹
        response = self.llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": eval_prompt}],
            temperature=self.temperature
        )
        
        evaluation_result = response.choices[0].message.content
        
        try:
            evaluation_data = json.loads(evaluation_result)
        except json.JSONDecodeError:
            print(f"âš ï¸ è©•ä¾¡çµæœã®JSONè§£æã«å¤±æ•—ã—ã¾ã—ãŸ")
            evaluation_data = {"error": "è©•ä¾¡çµæœã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ", "raw_result": evaluation_result}
        
        # è©•ä¾¡çµæœã‚’ä¿å­˜
        self.save_yaml({
            "evaluation": evaluation_data,
            "evaluated_at": datetime.datetime.now().isoformat(),
            "eval_prompt_preview": eval_prompt[:100] + "..." if len(eval_prompt) > 100 else eval_prompt,
            "task_type": "é¡§å®¢è¦ç´„"
        }, "evaluation.yaml")
        
        # MLFlowã«è¨˜éŒ²
        self._log_to_mlflow(answer, evaluation_data, prompt_data)
        
        return evaluation_data
    
    def _log_to_mlflow(self, answer: str, evaluation_data: Dict[str, Any], prompt_data: Dict[str, Any]):
        # å®Ÿé¨“è¨­å®š
        self._setup_mlflow_experiment()
        
        # run_nameã‚’æŒ‡å®šã—ã¦MLflowå®Ÿè¡Œé–‹å§‹
        run_name = f"{self.class_name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        print(f"ğŸš€ MLflowå®Ÿè¡Œé–‹å§‹: {run_name}")
        
        with mlflow.start_run(run_name=run_name):
            # ============================================================
            # 1. ã‚¯ãƒ©ã‚¹ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°
            # ============================================================
            mlflow.log_param("ã‚¯ãƒ©ã‚¹å", self.class_name)
            mlflow.log_param("ã‚¿ã‚¹ã‚¯ç¨®åˆ¥", "é¡§å®¢è¦ç´„")
            mlflow.log_param("å®Ÿè¡Œæ—¥æ™‚", datetime.datetime.now().isoformat())
            mlflow.log_param("ãƒ¢ãƒ‡ãƒ«å", "gpt-4o-mini")
            mlflow.log_param("Temperature", self.temperature)
            mlflow.log_param("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ", self.project_root)
            mlflow.log_param("å®Ÿé¨“ãƒ‘ã‚¹", self.experiment_path)
            
            # ============================================================
            # 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ­ã‚°
            # ============================================================
            if 'prompt' in prompt_data:
                mlflow.log_text(prompt_data['prompt'], f"{self.class_name}_ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ.txt")
            if 'eval_prompt' in prompt_data:
                mlflow.log_text(prompt_data['eval_prompt'], f"{self.class_name}_è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ.txt")
            
            # ============================================================
            # 3. ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã‚’ãƒ­ã‚°
            # ============================================================
            mlflow.log_text(answer, f"{self.class_name}_è¦ç´„.txt")
            mlflow.log_param("è¦ç´„æ–‡å­—æ•°", len(answer))
            
            # ============================================================
            # 4. è©•ä¾¡çµæœã‚’ãƒ­ã‚°ï¼ˆè¦ªã‚¯ãƒ©ã‚¹ã®è©•ä¾¡é …ç›® + å­ã‚¯ãƒ©ã‚¹ç‰¹æœ‰ã®è©•ä¾¡é …ç›®ï¼‰
            # ============================================================
            if isinstance(evaluation_data, dict):
                # è¦ªã‚¯ãƒ©ã‚¹ã®è©•ä¾¡é …ç›®ï¼ˆç°¡æ½”ã•ã€ä¸€è²«æ€§ï¼‰
                if 'ç°¡æ½”ã•' in evaluation_data:
                    mlflow.log_metric("ç°¡æ½”ã•", evaluation_data['ç°¡æ½”ã•'])
                if 'ä¸€è²«æ€§' in evaluation_data:
                    mlflow.log_metric("ä¸€è²«æ€§", evaluation_data['ä¸€è²«æ€§'])
                
                # å­ã‚¯ãƒ©ã‚¹ç‰¹æœ‰ã®è©•ä¾¡é …ç›®ï¼ˆè¦ç´„ã®æ­£ç¢ºæ€§ï¼‰
                if 'è¦ç´„ã®æ­£ç¢ºæ€§' in evaluation_data:
                    mlflow.log_metric("è¦ç´„ã®æ­£ç¢ºæ€§", evaluation_data['è¦ç´„ã®æ­£ç¢ºæ€§'])
                
                # ============================================================
                # 5. ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ã¨ãƒ­ã‚°
                # ============================================================
                scores = []
                if 'ç°¡æ½”ã•' in evaluation_data:
                    scores.append(evaluation_data['ç°¡æ½”ã•'])
                if 'ä¸€è²«æ€§' in evaluation_data:
                    scores.append(evaluation_data['ä¸€è²«æ€§'])
                if 'è¦ç´„ã®æ­£ç¢ºæ€§' in evaluation_data:
                    scores.append(evaluation_data['è¦ç´„ã®æ­£ç¢ºæ€§'])
                
                if scores:
                    avg_score = sum(scores) / len(scores)
                    mlflow.log_metric("ç·åˆã‚¹ã‚³ã‚¢", avg_score)
                    mlflow.log_metric("æœ€å°ã‚¹ã‚³ã‚¢", min(scores))
                    mlflow.log_metric("æœ€å¤§ã‚¹ã‚³ã‚¢", max(scores))
                
                # è©•ä¾¡ç†ç”±ãŒã‚ã‚Œã°ãƒ­ã‚°
                if 'è©•ä¾¡ç†ç”±' in evaluation_data:
                    mlflow.log_text(str(evaluation_data['è©•ä¾¡ç†ç”±']), f"{self.class_name}_è©•ä¾¡ç†ç”±.txt")
            
            print(f"âœ… MLflowå®Ÿè¡Œå®Œäº†: {run_name}")