{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f70dcfb2-d105-4f6d-a787-6bd8e59b0e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 環境セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cefc531e-f4f2-46ad-9689-82af88cee5d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install \\\n",
    "    langchain==0.3.27 \\\n",
    "    langchain-core==0.3.79 \\\n",
    "    langchain-text-splitters==0.3.11 \\\n",
    "    openai==2.3.0 \\\n",
    "    mlflow==3.4.0 \\\n",
    "    pandas==2.2.3 \\\n",
    "    pyyaml==6.0.2\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fdb6cdd-4eca-4dda-8c6b-dbf1e68e9eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from typing import List, Dict\n",
    "from openai import OpenAI\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "client = OpenAI()\n",
    "print(\"✓ 環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52f009bd-8b07-4c66-959f-2b6b56d156b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# プロンプトとテストデータ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a78b6499-cd51-4e34-babf-cd8b789579f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"./\"\n",
    "\n",
    "# ============================================================\n",
    "# すべてのYAMLファイル読み込み\n",
    "# ============================================================\n",
    "\n",
    "# プロンプトテンプレート\n",
    "with open(f\"{base_path}/prompts.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts_config = yaml.safe_load(f)\n",
    "\n",
    "# システムプロンプト\n",
    "with open(f\"{base_path}/system_prompts.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    system_prompts_config = yaml.safe_load(f)\n",
    "\n",
    "# 評価設定\n",
    "with open(f\"{base_path}/evaluation.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    evaluation_config = yaml.safe_load(f)\n",
    "\n",
    "# テストデータ\n",
    "with open(f\"{base_path}/test_data.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data_config = yaml.safe_load(f)\n",
    "\n",
    "# ============================================================\n",
    "# プロンプトテンプレート変換\n",
    "# ============================================================\n",
    "\n",
    "prompt_versions = {}\n",
    "prompt_metadata = {}\n",
    "\n",
    "for version_name, config in prompts_config.items():\n",
    "    prompt_versions[version_name] = PromptTemplate.from_template(config[\"template\"])\n",
    "    prompt_metadata[version_name] = {\n",
    "        \"name\": config.get(\"name\", \"\"),\n",
    "        \"description\": config.get(\"description\", \"\"),\n",
    "        \"version\": config.get(\"version\", \"\"),\n",
    "        \"created_date\": config.get(\"created_date\", \"\"),\n",
    "        \"author\": config.get(\"author\", \"\")\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# テストデータ取得\n",
    "# ============================================================\n",
    "\n",
    "test_customers = test_data_config[\"test_cases\"]\n",
    "test_metadata = test_data_config[\"metadata\"]\n",
    "\n",
    "# ============================================================\n",
    "# 評価プロンプトテンプレート変換\n",
    "# ============================================================\n",
    "\n",
    "eval_config = evaluation_config[\"summary_evaluation\"]\n",
    "evaluation_prompt_template = PromptTemplate.from_template(eval_config[\"template\"])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"✓ すべての設定読み込み完了\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n【プロンプト】{len(prompt_versions)}個\")\n",
    "for name in prompt_versions.keys():\n",
    "    print(f\"  - {name}: {prompt_metadata[name]['name']}\")\n",
    "\n",
    "print(f\"\\n【テストデータ】{len(test_customers)}件\")\n",
    "print(f\"  データセット: {test_metadata['dataset_name']}\")\n",
    "print(f\"  バージョン: {test_metadata['version']}\")\n",
    "\n",
    "print(f\"\\n【システムプロンプト】{len(system_prompts_config)}個\")\n",
    "for name, config in system_prompts_config.items():\n",
    "    print(f\"  - {name}: {config['name']}\")\n",
    "\n",
    "print(f\"\\n【評価設定】\")\n",
    "print(f\"  評価名: {eval_config['name']}\")\n",
    "print(f\"  モデル: {eval_config['model']}\")\n",
    "print(f\"  Temperature: {eval_config['temperature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa185a35-abcc-48c9-85c5-7e44b5ad9c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 推論関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e4ebbe-e31d-4d65-b66f-96557ffab62a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_summary(\n",
    "    customer: Dict, \n",
    "    prompt_template: PromptTemplate,\n",
    "    system_prompt: str,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.3\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    PromptTemplateとシステムプロンプトを使って要約生成\n",
    "    \"\"\"\n",
    "    # LangChainのPromptTemplateで整形\n",
    "    prompt = prompt_template.format(**customer)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"✓ 推論関数定義完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bdb1f25-5088-428e-a95e-22bd53810f1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 評価関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121d9322-84ab-4d61-9c35-0f791e3d7ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_summary(customer: Dict, summary: str) -> Dict:\n",
    "    \"\"\"\n",
    "    YAML設定に基づいて生成された要約を評価\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"score\": float (0.0-1.0),\n",
    "            \"reasoning\": str,\n",
    "            \"metrics\": {\n",
    "                \"conciseness\": float,\n",
    "                \"clarity\": float,\n",
    "                \"actionability\": float\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # 評価プロンプト生成\n",
    "    eval_prompt = evaluation_prompt_template.format(\n",
    "        customer_name=customer['name'],\n",
    "        customer_age=customer['age'],\n",
    "        customer_occupation=customer['occupation'],\n",
    "        customer_purchase_history=customer['purchase_history'],\n",
    "        customer_inquiries=customer['inquiries'],\n",
    "        customer_notes=customer['notes'],\n",
    "        generated_summary=summary\n",
    "    )\n",
    "    \n",
    "    # 評価実行\n",
    "    response = client.chat.completions.create(\n",
    "        model=eval_config[\"model\"],\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": system_prompts_config[\"evaluation_judge\"][\"content\"]\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": eval_prompt}\n",
    "        ],\n",
    "        temperature=eval_config[\"temperature\"],\n",
    "        response_format={\"type\": eval_config[\"response_format\"]}\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    \n",
    "    # 総合スコア計算（平均を0-1に正規化）\n",
    "    total_score = (\n",
    "        result[\"conciseness\"] + \n",
    "        result[\"clarity\"] + \n",
    "        result[\"actionability\"]\n",
    "    ) / 30.0\n",
    "    \n",
    "    return {\n",
    "        \"score\": total_score,\n",
    "        \"reasoning\": result.get(\"reasoning\", \"\"),\n",
    "        \"metrics\": {\n",
    "            \"conciseness\": result[\"conciseness\"] / 10.0,\n",
    "            \"clarity\": result[\"clarity\"] / 10.0,\n",
    "            \"actionability\": result[\"actionability\"] / 10.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"✓ 評価関数定義完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e122b10-a95a-40ff-a639-00ad06d0b0f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 要約と評価を実施しMLFlowに記録"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eafdcf0-31d6-4f75-9845-2af15493ee7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MLflow実験設定\n",
    "mlflow.set_experiment(experiment_id=\"2762244084694140\")\n",
    "\n",
    "system_prompt = system_prompts_config[\"default\"][\"content\"]\n",
    "\n",
    "for version_name, prompt_template in prompt_versions.items():\n",
    "    metadata = prompt_metadata[version_name]\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{version_name}_{metadata['name']}\"):\n",
    "        \n",
    "        # ============================================================\n",
    "        # 1. プロンプトメタデータをログ\n",
    "        # ============================================================\n",
    "        mlflow.log_param(\"プロンプトバージョン\", version_name)\n",
    "        mlflow.log_param(\"プロンプト名\", metadata['name'])\n",
    "        mlflow.log_param(\"説明\", metadata['description'])\n",
    "        mlflow.log_param(\"バージョン\", metadata['version'])\n",
    "        mlflow.log_param(\"作成日\", metadata['created_date'])\n",
    "        mlflow.log_param(\"作成者\", metadata['author'])\n",
    "        \n",
    "        # ============================================================\n",
    "        # 2. システムプロンプトをログ\n",
    "        # ============================================================\n",
    "        mlflow.log_param(\"システムプロンプト\", system_prompts_config[\"default\"][\"name\"])\n",
    "        mlflow.log_text(system_prompt, \"システムプロンプト.txt\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 3. プロンプトテンプレートをログ\n",
    "        # ============================================================\n",
    "        mlflow.log_text(prompt_template.template, f\"{version_name}_プロンプト.txt\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 4. テストデータメタデータをログ\n",
    "        # ============================================================\n",
    "        mlflow.log_param(\"テストデータセット\", test_metadata['dataset_name'])\n",
    "        mlflow.log_param(\"テストデータ件数\", len(test_customers))\n",
    "        mlflow.log_param(\"テストデータバージョン\", test_metadata['version'])\n",
    "        \n",
    "        # ============================================================\n",
    "        # 5. 評価設定をログ\n",
    "        # ============================================================\n",
    "        mlflow.log_param(\"評価モデル\", eval_config['model'])\n",
    "        mlflow.log_param(\"評価Temperature\", eval_config['temperature'])\n",
    "        mlflow.log_param(\"評価プロンプト名\", eval_config['name'])\n",
    "        mlflow.log_text(evaluation_prompt_template.template, \"評価プロンプト.txt\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # 6. 各顧客で評価実行\n",
    "        # ============================================================\n",
    "        version_scores = []\n",
    "        customer_results = []\n",
    "        \n",
    "        for customer in test_customers:\n",
    "            # 要約生成\n",
    "            summary = generate_summary(customer, prompt_template, system_prompt)\n",
    "            \n",
    "            # 評価実行\n",
    "            evaluation = evaluate_summary(customer, summary)\n",
    "            \n",
    "            # 結果記録\n",
    "            result = {\n",
    "                \"顧客名\": customer[\"name\"],\n",
    "                \"要約\": summary,\n",
    "                \"要約文字数\": len(summary),\n",
    "                \"総合スコア\": evaluation[\"score\"],\n",
    "                \"簡潔性\": evaluation[\"metrics\"][\"conciseness\"],\n",
    "                \"明瞭性\": evaluation[\"metrics\"][\"clarity\"],\n",
    "                \"実用性\": evaluation[\"metrics\"][\"actionability\"],\n",
    "                \"評価理由\": evaluation[\"reasoning\"]\n",
    "            }\n",
    "            \n",
    "            customer_results.append(result)\n",
    "            version_scores.append(evaluation[\"score\"])\n",
    "            \n",
    "            # 顧客ごとの詳細結果をログ\n",
    "            mlflow.log_metric(f\"スコア_{customer['customer_id']}\", evaluation[\"score\"])\n",
    "        \n",
    "        # ============================================================\n",
    "        # 7. バージョンごとの集計指標をログ\n",
    "        # ============================================================\n",
    "        avg_score = sum(version_scores) / len(version_scores)\n",
    "        avg_conciseness = sum(r[\"簡潔性\"] for r in customer_results) / len(customer_results)\n",
    "        avg_clarity = sum(r[\"明瞭性\"] for r in customer_results) / len(customer_results)\n",
    "        avg_actionability = sum(r[\"実用性\"] for r in customer_results) / len(customer_results)\n",
    "        avg_length = sum(r[\"要約文字数\"] for r in customer_results) / len(customer_results)\n",
    "        \n",
    "        mlflow.log_metric(\"平均スコア\", avg_score)\n",
    "        mlflow.log_metric(\"平均_簡潔性\", avg_conciseness)\n",
    "        mlflow.log_metric(\"平均_明瞭性\", avg_clarity)\n",
    "        mlflow.log_metric(\"平均_実用性\", avg_actionability)\n",
    "        mlflow.log_metric(\"平均_要約文字数\", avg_length)\n",
    "        mlflow.log_metric(\"最小スコア\", min(version_scores))\n",
    "        mlflow.log_metric(\"最大スコア\", max(version_scores))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "llm",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
